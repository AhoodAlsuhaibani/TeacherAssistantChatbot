{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7882be9030ae41b4ac4d5f20c68abc94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d962d01056dc4fd0989b33c70a3e7aa1",
              "IPY_MODEL_d85083b9b32c434dad8e653fdba839b3",
              "IPY_MODEL_4358bd48996643628a3dec332c3193ec"
            ],
            "layout": "IPY_MODEL_a6efe4ddbc6948c4babda19fcd8200d4"
          }
        },
        "d962d01056dc4fd0989b33c70a3e7aa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33eea9753dc04de9b67e19eededd955a",
            "placeholder": "​",
            "style": "IPY_MODEL_5190647e118b4305b4ca01fd40bee78d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d85083b9b32c434dad8e653fdba839b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39a88e9607514d8da926d3426ab7ba11",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5fcdac78bab449b791895a7f110728c5",
            "value": 8
          }
        },
        "4358bd48996643628a3dec332c3193ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9374c5e4565d44a6b45890ffab74b07d",
            "placeholder": "​",
            "style": "IPY_MODEL_81ad7a0642ac4e5d8c35b10f3aaabf58",
            "value": " 8/8 [01:14&lt;00:00,  8.37s/it]"
          }
        },
        "a6efe4ddbc6948c4babda19fcd8200d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33eea9753dc04de9b67e19eededd955a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5190647e118b4305b4ca01fd40bee78d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39a88e9607514d8da926d3426ab7ba11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fcdac78bab449b791895a7f110728c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9374c5e4565d44a6b45890ffab74b07d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81ad7a0642ac4e5d8c35b10f3aaabf58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhoodAlsuhaibani/TeacherAssistantChatbot/blob/main/Teacher's_Assistant_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8aosW8xGPAw",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers transformers accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl pacmap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlzxWi3fGTIW",
        "outputId": "778825e5-c395-4023-e763-98c867ab4490",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.0.38)\n",
            "Collecting langchain-community\n",
            "  Using cached langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.6)\n",
            "Collecting langchain<0.3.0,>=0.2.0 (from langchain-community)\n",
            "  Using cached langchain-0.2.1-py3-none-any.whl (973 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain-community)\n",
            "  Using cached langchain_core-0.2.1-py3-none-any.whl (308 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.63)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.0->langchain-community)\n",
            "  Using cached langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Installing collected packages: langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.1.52\n",
            "    Uninstalling langchain-core-0.1.52:\n",
            "      Successfully uninstalled langchain-core-0.1.52\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.0.2\n",
            "    Uninstalling langchain-text-splitters-0.0.2:\n",
            "      Successfully uninstalled langchain-text-splitters-0.0.2\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.1.20\n",
            "    Uninstalling langchain-0.1.20:\n",
            "      Successfully uninstalled langchain-0.1.20\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.0.38\n",
            "    Uninstalling langchain-community-0.0.38:\n",
            "      Successfully uninstalled langchain-community-0.0.38\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ragatouille 0.0.8.post2 requires langchain<0.2.0,>=0.1.0, but you have langchain 0.2.1 which is incompatible.\n",
            "ragatouille 0.0.8.post2 requires langchain_core<0.2.0,>=0.1.4, but you have langchain-core 0.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.2.1 langchain-community-0.2.1 langchain-core-0.2.1 langchain-text-splitters-0.2.0\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.6)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.1)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.63)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "3PdeGtB-GglF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOxz1SwYGgR6",
        "outputId": "44c18740-919b-4387-9346-821774a6521a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.6)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.1)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.63)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOfw3H6kMXG8",
        "outputId": "d2fa7981-a4f0-4e58-86f2-916db965eeef",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekdpxJUrGfA9",
        "outputId": "469f64bb-7849-40e5-abe4-9c23a25d088a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndv-4j7Czonb",
        "outputId": "d09fbc40-bb5e-4f0f-8356-0cb1ebe9ef91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "qhSrtfNCPa06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_docs=[]\n",
        "from langchain_community.document_loaders import NotebookLoader\n",
        "for file in os.listdir(\"/content/drive/MyDrive/CODES\"):\n",
        "  if file.endswith(\".ipynb\"):\n",
        "    ipynb_path=\"/content/drive/MyDrive/CODES/\"+file\n",
        "    loader = NotebookLoader(\n",
        "    ipynb_path,\n",
        "    include_outputs=True,\n",
        "    max_output_length=20,\n",
        "    remove_newline=True,\n",
        ")\n",
        "    code_docs.extend(loader.load())"
      ],
      "metadata": {
        "id": "0JfIHtGAGP75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_docs=[]\n",
        "for file in os.listdir(\"/content/drive/MyDrive/PDFS \"):\n",
        "  if file.endswith(\".pdf\"):\n",
        "    pdf_path=\"/content/drive/MyDrive/PDFS /\"+file\n",
        "    loader=PyPDFLoader(pdf_path)\n",
        "    pdf_docs.extend(loader.load())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qUvKWEfOX0L",
        "outputId": "6afb674d-d7e6-4d65-cb38-764c0ff556ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 9 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 20 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 23 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 25 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 74 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 83 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 86 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 88 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 95 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 132 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 138 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 140 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 188 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 190 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 201 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 204 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 206 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 213 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 227 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 235 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 237 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 239 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 241 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 249 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 251 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 253 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 263 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 265 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 267 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 281 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 283 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 285 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 288 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 290 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 293 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 295 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 300 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 311 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 313 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 320 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 325 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 331 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 333 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 376 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 383 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 387 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 414 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 420 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 422 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 424 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 442 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 449 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 457 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 468 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 493 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 495 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 497 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 560 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 579 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 603 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 605 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 611 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 613 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 674 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 676 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 678 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 684 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 694 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 696 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 733 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 772 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 798 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 902 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 909 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 986 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 21 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 35 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 37 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 39 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 41 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 43 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 53 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 55 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 65 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 67 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 69 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 72 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 74 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 87 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 145 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 224 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 252 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 255 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 257 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 266 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 268 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 274 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 276 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 303 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 313 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 319 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 321 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 324 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 326 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 342 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 362 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 364 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 379 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 385 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 419 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 423 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 445 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 504 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 508 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 514 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 516 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 518 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 528 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 573 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 592 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 596 0 (offset 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "%pip install --upgrade --quiet  youtube-transcript-api\n",
        "\n",
        "vid_docs=[]\n",
        "vid_list=[\"https://www.youtube.com/watch?v=HXV3zeQKqGY\",\"https://www.youtube.com/watch?v=_uQrJ0TkZlc\",\"https://www.youtube.com/watch?v=JonnQuQDThc\",\"https://www.youtube.com/watch?v=d2kxUVwWWwU\",\"https://www.youtube.com/watch?v=IA3WxTTPXqQ\",\"https://www.youtube.com/watch?v=igKTO7lQxNo\",\"https://www.youtube.com/watch?v=r-uOLxNrNk8\"]\n",
        "\n",
        "for vid in vid_list:\n",
        "  loader=YoutubeLoader.from_youtube_url(\n",
        "    vid, add_video_info=False\n",
        ")\n",
        "  vid_docs.extend(loader.load())"
      ],
      "metadata": {
        "id": "-ADTzYx4RPCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4m6TwDJ9-9L"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "NOTEBOOK_SEPARATORS = [\n",
        "    \"\\n# \",        # Headings\n",
        "    \"\\n## \",       # Subheadings\n",
        "    \"\\n### \",      # Sub-subheadings\n",
        "    \"\\n---\\n\",     # Horizontal rule\n",
        "    \"\\n\\n\",        # Double newline\n",
        "    \"\\n\",          # Single newline\n",
        "    \" \",           # Space\n",
        "    \"\",            # Empty string\n",
        "]\n",
        "\n",
        "\n",
        "PDF_SEPARATORS = [\"\\f\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hvIL2jO9-9M"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "from typing import Optional, List, Tuple\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=PDF_SEPARATORS,\n",
        "    )\n",
        "\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "\n",
        "docs_processed_pdf = split_documents(\n",
        "   512,\n",
        "    pdf_docs,\n",
        "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "\n",
        "docs_processed_vid = split_documents(\n",
        "   512,\n",
        "    vid_docs,\n",
        "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
        ")\n"
      ],
      "metadata": {
        "id": "kmoSHh8_VY7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=NOTEBOOK_SEPARATORS,\n",
        "    )\n",
        "\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "\n",
        "docs_processed_code = split_documents(\n",
        "   512,\n",
        "    code_docs,\n",
        "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
        ")\n"
      ],
      "metadata": {
        "id": "vIacfbSFVYv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs_processed_pdf[0])\n",
        "print(docs_processed_vid[0])\n",
        "print(docs_processed_code[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HpzwemX0wUN",
        "outputId": "f38765da-79a1-4e16-b0b3-7b3b66c38e28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='1 - V 1.0/05032024  \\nProgram Project  \\nProgram Identification  \\nProgram ID:  T5 \\nProgram Title: Introduction to AI, Data Analytics & SQL  \\n \\nProgram Information:  \\nProject Title:  Railway System Database Management System    \\n \\nDescription  \\n \\nThe railway management system project can help make the process of planning trips, booking tickets, reservations and last -minute cancellations more convenient. \\nThe system will streamline the process for users, which will also help retain them as users. Create a database he dataset for this project will contain essential \\ndetails, such as:    \\na) Train information like ( Train number , speed, …etc  \\nb) Station information like ( Station code , Station name , …etc)  \\nc) Schedule information like (Trip code, departure city, arrival city, Departure time , Distance , Price    …etc)  \\nd) Traveler information like (Name, phone number, age, ….etc)  \\ne) Ticket information like (Date, trip number, client number, …etc)  \\n \\nThe idea of this project is for users to develop the database for them to perform the following tasks:    \\na) Book their tickets or cancel booked tickets.    \\nb) Check their fares before booking tickets and checking their booked tickets.    \\nc) Check the schedule for available trains.' metadata={'source': '/content/drive/MyDrive/PDFS /Project T5-W1-B3-032024 (1).pdf', 'page': 0, 'start_index': 4}\n",
            "page_content=\"SQL Tutorial - Full Database Course for Beginners In this course I’m going to teach you everything you \\nneed to know to get started using SQL. Now SQL is a language which is used to interact \\nwith relational database management systems. And a relational database management system is basically just a software application which we can \\nuse to create and manage different databases. And so, we're going to talk about \\nall of this stuff in this course. We’re going to start off with the basics. So, we'll just look at what is a database. We’ll talk about the different types of databases. We'll talk about what SQL is and what it actually \\nmeans and how you can use it to create databases. And then we're going to go ahead \\nand we're going to install something called a relational database management system. Which like I said, is just software \\nthat we can use to manage a database. We're going to install a relational database \\nmanagement system called MySQL. And MySQL is one of the most popular database \\nmanagement systems for beginners. And also, just in general. So, MySQL is a great first system to learn. And so, once we have that all install, \\nthen we'll start writing SQL. So, we can write out little SQL code, \\nlittle queries in order to create databases \\nand create database tables and, you know, input information, \\nretrieve information. And then we're going to get into writing SQL queries. And queries are used to query a database. So, we'll create a database. We’ll populate it with information. And I’ll show you guys how you can write these little \\nSQL queries to get specific pieces of information. So, we'll start off with the basics \\nand we'll just learn all of the fundamentals. And then I’m going to show you guys \\nsome more advanced techniques to getting information out of a database. And finally, I’m going to show you guys \\nhow you can actually design database schemas. So, a database schema is basically just like all of the\" metadata={'source': 'HXV3zeQKqGY', 'start_index': 0}\n",
            "page_content='\\'markdown\\' cell: \\'[\\'**End-to-end Machine Learning project**\\', \\'\\', \"*Let\\'s  predict median house values in Californian districts based on features from these districts.*\"]\\'\\n\\n\\'markdown\\' cell: \\'[\"# Let\\'s import a few common modules\"]\\'\\n\\n\\'markdown\\' cell: \\'[\\'- MatplotLib plots figures inline\\', \\'- Create a function to save the figures\\', \\'- Check that Python installed version to be 3.5 or later\\', \\'- Check that Scikit-Learn ≥0.20.\\']\\'' metadata={'source': '/content/drive/MyDrive/CODES/T5B3ML001N01_032024V1.ipynb', 'start_index': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VECTORDB = FAISS.from_documents(\n",
        "    docs_processed_pdf + docs_processed_code + docs_processed_vid, embedding_model,distance_strategy=DistanceStrategy.COSINE\n",
        ")"
      ],
      "metadata": {
        "id": "GW8RhBxmajyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VECTORDB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNsbG-63cEoN",
        "outputId": "4d845677-31a8-4828-fbf4-2d63bd61f0f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x78ff0462f040>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FAISS.save_local(VECTORDB, \"faiss_index\")"
      ],
      "metadata": {
        "id": "Y2iEnyOecJja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VECTORDB = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n"
      ],
      "metadata": {
        "id": "xULvQ-AFOigl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what is NLP?\"\n",
        "print(f\"\\nStarting retrieval for {user_query=}...\")\n",
        "retrieved_docs = VECTORDB.similarity_search(query=user_query, k=5)\n",
        "print(\n",
        "    \"\\n==================================Top document==================================\"\n",
        ")\n",
        "print(retrieved_docs[0].page_content)\n",
        "print(\"==================================Metadata==================================\")\n",
        "print(retrieved_docs[0].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4KbV-_WJupk",
        "outputId": "82333fb3-c052-4f54-f885-dbb2a9ee787c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting retrieval for user_query='what is NLP?'...\n",
            "\n",
            "==================================Top document==================================\n",
            "for people who are not aware about nlp i would like to give a quick introduction about what nlp is and why is it important right so nlp is the abbreviation for natural language processing and nlp is a combination of using human language with a bit of programming and artificial intelligence so that we are able to convert text into a format that can be you know run through algorithms and used for prediction right so we are converting the unstructured text data into structured data and then gaining insights this entire process is called nlp now you might have\n",
            "==================================Metadata==================================\n",
            "{'source': 'igKTO7lQxNo', 'start_index': 352664}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX_ORK4l9-9N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "7882be9030ae41b4ac4d5f20c68abc94",
            "d962d01056dc4fd0989b33c70a3e7aa1",
            "d85083b9b32c434dad8e653fdba839b3",
            "4358bd48996643628a3dec332c3193ec",
            "a6efe4ddbc6948c4babda19fcd8200d4",
            "33eea9753dc04de9b67e19eededd955a",
            "5190647e118b4305b4ca01fd40bee78d",
            "39a88e9607514d8da926d3426ab7ba11",
            "5fcdac78bab449b791895a7f110728c5",
            "9374c5e4565d44a6b45890ffab74b07d",
            "81ad7a0642ac4e5d8c35b10f3aaabf58"
          ]
        },
        "outputId": "364af196-514a-43b5-feb0-c378a1ea81fb",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7882be9030ae41b4ac4d5f20c68abc94"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-alpha\"  #\"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    READER_MODEL_NAME, quantization_config=bnb_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=1024,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Pipeline\n",
        "import shelve\n",
        "\n",
        "def get_chat_history(chat_id):\n",
        "  \"\"\"\n",
        "  Retrieves the last N questions and answers from the chat history database.\n",
        "\n",
        "  Args:\n",
        "      chat_id (int): The unique identifier of the chat.\n",
        "\n",
        "  Returns:\n",
        "      str: A formatted string containing the last N questions and answers.\n",
        "  \"\"\"\n",
        "\n",
        "  with shelve.open('chat_history') as db:\n",
        "    try:\n",
        "      history = db[str(chat_id)]\n",
        "      formatted_history = \"\\n\".join(history[-4:])  # Get the last 4 entries\n",
        "    except KeyError:\n",
        "      formatted_history = \"No chat history available yet.\"\n",
        "\n",
        "  return formatted_history\n",
        "\n",
        "def update_chat_history(chat_id, question):\n",
        "  \"\"\"\n",
        "  Updates the chat history database with the latest question.\n",
        "\n",
        "  Maintains a maximum of N entries.\n",
        "\n",
        "  Args:\n",
        "      chat_id (int): The unique identifier of the chat.\n",
        "      question (str): The user's latest question.\n",
        "  \"\"\"\n",
        "\n",
        "  with shelve.open('chat_history') as db:\n",
        "    try:\n",
        "      history = db.get(str(chat_id), [])\n",
        "      history.append(question)\n",
        "      history = history[-4:]  # Keep only the last 4 entries\n",
        "      db[str(chat_id)] = history\n",
        "    except KeyError:\n",
        "      db[str(chat_id)] = [question]\n",
        "\n",
        "prompt_in_chat_format = [\n",
        "  {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"Considering ONLY the following user question, provide a comprehensive answer.\n",
        "    Respond only to the question asked, response should be concise and relevant.\"\"\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"\"\"Context:\n",
        "    {context}\n",
        "    ---\n",
        "    Now here is the question you need to answer.\n",
        "\n",
        "    Question: {question}\"\"\"\n",
        "  },\n",
        "]\n",
        "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "print(RAG_PROMPT_TEMPLATE)\n"
      ],
      "metadata": {
        "id": "SxEtxCPzkN48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa513d2-fa8f-470a-ed0d-d9e90e46c973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "Considering ONLY the following user question, provide a comprehensive answer.\n",
            "    Respond only to the question asked, response should be concise and relevant.</s>\n",
            "<|user|>\n",
            "Context:\n",
            "    {context}\n",
            "    ---\n",
            "    Now here is the question you need to answer.\n",
            "\n",
            "    Question: {question}</s>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ragatouille"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkJ-CZcsKJHN",
        "outputId": "d240687b-6755-4c39-923a-e6a7621f971f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ragatouille in /usr/local/lib/python3.10/dist-packages (0.0.8.post2)\n",
            "Requirement already satisfied: colbert-ai==0.2.19 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (0.2.19)\n",
            "Requirement already satisfied: faiss-cpu<2.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (1.8.0)\n",
            "Requirement already satisfied: fast-pytorch-kmeans==0.2.0.1 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (0.2.0.1)\n",
            "Collecting langchain<0.2.0,>=0.1.0 (from ragatouille)\n",
            "  Using cached langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
            "Collecting langchain_core<0.2.0,>=0.1.4 (from ragatouille)\n",
            "  Using cached langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "Requirement already satisfied: llama-index>=0.7 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (0.10.40)\n",
            "Requirement already satisfied: onnx<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (1.16.1)\n",
            "Requirement already satisfied: sentence-transformers<3.0.0,>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (2.7.0)\n",
            "Requirement already satisfied: srsly==2.4.8 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (2.4.8)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.36.2 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (4.41.1)\n",
            "Requirement already satisfied: voyager<3.0.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (2.0.6)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (2.9.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (2.19.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (2.2.5)\n",
            "Requirement already satisfied: git-python in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (1.0.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (1.0.1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (1.11.1.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (4.66.4)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (5.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fast-pytorch-kmeans==0.2.0.1->ragatouille) (1.25.2)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from fast-pytorch-kmeans==0.2.0.1->ragatouille) (11.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from srsly==2.4.8->ragatouille) (2.0.10)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.6.6)\n",
            "Collecting langchain-community<0.1,>=0.0.38 (from langchain<0.2.0,>=0.1.0->ragatouille)\n",
            "  Using cached langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.0->ragatouille)\n",
            "  Using cached langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.1.63)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (8.3.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core<0.2.0,>=0.1.4->ragatouille) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core<0.2.0,>=0.1.4->ragatouille) (23.2)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.7->ragatouille) (0.2.5)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.7->ragatouille) (0.1.12)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.40 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.7->ragatouille) (0.10.40)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.7->ragatouille) (0.1.10)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.7->ragatouille) (0.1.6)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.7->ragatouille) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.7->ragatouille) (0.1.21)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.7->ragatouille) (0.1.6)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.7->ragatouille) (0.1.6)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.7->ragatouille) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.7->ragatouille) (0.1.23)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.7->ragatouille) (0.1.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx<2.0.0,>=1.15.0->ragatouille) (3.20.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.2.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (0.23.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->ragatouille) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->ragatouille) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->ragatouille) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->ragatouille) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core<0.2.0,>=0.1.4->ragatouille) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.0->ragatouille) (3.10.3)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index>=0.7->ragatouille) (1.30.4)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (1.0.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (1.6.0)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (2.0.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (0.7.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (1.14.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (4.2.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (0.0.26)\n",
            "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index>=0.7->ragatouille) (0.4.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.0->ragatouille) (3.0.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.70.16)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (3.0.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->ragatouille) (2.1.5)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (from git-python->colbert-ai==0.2.19->ragatouille) (3.1.43)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (3.5.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->ragatouille) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (2.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index>=0.7->ragatouille) (1.7.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->ragatouille) (1.0.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython->git-python->colbert-ai==0.2.19->ragatouille) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (1.2.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19->ragatouille) (5.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.40->llama-index>=0.7->ragatouille) (1.16.0)\n",
            "Installing collected packages: langchain_core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: langchain_core\n",
            "    Found existing installation: langchain-core 0.2.1\n",
            "    Uninstalling langchain-core-0.2.1:\n",
            "      Successfully uninstalled langchain-core-0.2.1\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.2.0\n",
            "    Uninstalling langchain-text-splitters-0.2.0:\n",
            "      Successfully uninstalled langchain-text-splitters-0.2.0\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.2.1\n",
            "    Uninstalling langchain-community-0.2.1:\n",
            "      Successfully uninstalled langchain-community-0.2.1\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.2.1\n",
            "    Uninstalling langchain-0.2.1:\n",
            "      Successfully uninstalled langchain-0.2.1\n",
            "Successfully installed langchain-0.1.20 langchain-community-0.0.38 langchain-text-splitters-0.0.2 langchain_core-0.1.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drpu6xY2KK2H",
        "outputId": "f0b1039a-9e53-4610-affe-a9b4c8f081eb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List, Tuple\n",
        "from langchain.docstore.document import Document as LangchainDocument\n"
      ],
      "metadata": {
        "id": "zq0bQabKOzcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n11zYRfn9-9O"
      },
      "outputs": [],
      "source": [
        "from transformers import Pipeline\n",
        "\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: Pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
        "    )\n",
        "\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    # Redact an answer\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, relevant_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZTC1FtX9-9P",
        "outputId": "ea326979-de11-4f7a-8732-b4dfd14772b2",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Retrieving documents...\n",
            "=> Reranking documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Generating answer...\n"
          ]
        }
      ],
      "source": [
        "question = \"What is NLP types?\"\n",
        "\n",
        "\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(\n",
        "   question, READER_LLM, VECTORDB, reranker=RERANKER\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwW0oqhZ9-9P",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1860937-1da8-438b-fa0e-2bd2df7abd93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================Answer==================================\n",
            "Answer: NLP types refer to the different analysis methods used in natural language processing. These include lexical analysis, syntactic analysis, semantic analysis, discourse integration, and pragmatic analysis. Each type involves specific techniques and goals to identify and analyze the structure of words, arrange them for grammar, determine their exact meaning, ensure meaningfulness, consider relationships between sentences, and reinterpret language based on real-world knowledge.\n",
            "==================================Source docs==================================\n",
            "Document 0------------------------------------------------------------\n",
            "NLP Analysis Types-Lexical Analysis: Involves identifying and analyzing the structure of words, dividing text into paragraphs, sentences, and words. Lexicon represents the collection of words and phrases in a language.-Syntactic Analysis: Analyzes words in a sentence for grammar and arranges them to show relationships among words. Incorrect sentences like “The school goes to boy” are rejected by English syntactic analyzers.-Semantic Analysis: Determines the exact meaning or dictionary meaning from the text, ensuring meaningfulness by mapping syntactic structures and objects in the task domain. Sentence examples like “hot ice-cream” are disregarded.-Discourse Integration: Considers the meaning of each sentence in relation to the sentence before it and after it, ensuring coherence and continuity in the overall text.-Pragmatic Analysis: Reinterprets what was said to understand what it actually meant, requiring real-world knowledge to derive aspects of language beyond literal meanings.\n",
            "10\n",
            "Document 1------------------------------------------------------------\n",
            "5\n",
            "What is Natural Language Processing?Natural language processing (NLP) is the intersection of computer science, linguistics and machine learning.-Natural language processing, or NLP, combines computational linguistics—rule-based modeling of human language—with statistical and machine learning models to enable computers and digital devices to recognize, understand and generate text and speech.-Applications of NLP techniques include voice assistants like Amazon’s Alexa and Apple’s Siri, but also things like machine translation and text- filtering.-   The ultimate goal of NLP is all about making computers understand and generate human language.\n",
            "Document 2------------------------------------------------------------\n",
            "41Natural Language Processing (NLP)\n",
            "•Convert words and characters \n",
            "into numbers\n",
            "•Conversion of characters and words into numbers:\n",
            "•Bag of Words\n",
            "•Frequency Matrix\n",
            "•TF/IDFF\n",
            "•Word Embeddings\n",
            "•Stop Words and Stemming\n",
            "Document 3------------------------------------------------------------\n",
            "NLP Terminology\n",
            "7There are some key fundamentals of natural language:1.Syntax - This refers to the rules and structures of the arrangement of words to create asentence.2.Semantics - This refers to the meaning behind words, phrases and sentences in language.3.Morphology - This refers to the study of the actual structure of words and how they are formedfrom smaller units called morphemes.4.Pragmatics - This is the study of how context plays a big role in the interpretation of language, for example, tone.5.Phonology/Phoneme - This refers to the study of sounds in language, and how the distinct unitsare formed together to combine words.6.Ambiguity - This refers to words or sentences with multiple interpretations.7.Polysemy - This refers to words with multiple related meanings.\n",
            "Document 4------------------------------------------------------------\n",
            "an interface between this is not the interface that we use it's not the user interface this is a bridge between the nlp model and the chat window now let's talk about the nlp model so we can talk about why we require this interface first of all right now when you're talking about the nlp model nlp stands for natural language processing this is a model which is again built either using a neural network or machine learning algorithms in a way where it has the capability to first of all have certain points of data to use to\n"
          ]
        }
      ],
      "source": [
        "print(\"==================================Answer==================================\")\n",
        "print(f\"{answer}\")\n",
        "print(\"==================================Source docs==================================\")\n",
        "for i, doc in enumerate(relevant_docs):\n",
        "    print(f\"Document {i}------------------------------------------------------------\")\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"how to remove tashkeel from arabic?\"\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(\n",
        "    question, READER_LLM, VECTORDB, reranker=RERANKER\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "5yftFIqqK1_4",
        "outputId": "c5799b4a-948c-4802-a898-6ea81f4cb9f3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Retrieving documents...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-c724c1cd79c0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"how to remove tashkeel from arabic?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m answer, relevant_docs = answer_with_rag(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mREADER_LLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVECTORDB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreranker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRERANKER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-32-174efff23007>\u001b[0m in \u001b[0;36manswer_with_rag\u001b[0;34m(question, llm, knowledge_index, reranker, num_retrieved_docs, num_docs_final)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Gather documents with retriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=> Retrieving documents...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     relevant_docs = knowledge_index.similarity_search(\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_retrieved_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36msimilarity_search\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mDocuments\u001b[0m \u001b[0mmost\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \"\"\"\n\u001b[0;32m--> 530\u001b[0;31m         docs_and_scores = self.similarity_search_with_score(\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfetch_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36msimilarity_search_with_score\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mL2\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mLower\u001b[0m \u001b[0mscore\u001b[0m \u001b[0mrepresents\u001b[0m \u001b[0mmore\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \"\"\"\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         docs = self.similarity_search_with_score_by_vector(\n\u001b[1;32m    404\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36m_embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_embed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/huggingface.py\u001b[0m in \u001b[0;36membed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mEmbeddings\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \"\"\"\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/huggingface.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_multi_process_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_multi_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_multi_process_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mstart_multi_process_pool\u001b[0;34m(self, target_devices)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pooling_mode_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"cls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"max\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             ):\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0mpooling_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m                 \u001b[0mpooling_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpooling_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pooling_mode_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 model_card = model_card.replace(\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mfds_to_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pBEqExiK00q",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebf03ff-93cb-4162-c17e-e54aa77a7418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================Answer==================================\n",
            "To remove tashkeel (diacritical marks) from Arabic text, you can use the `strip_tashkeel()` function provided by the Python library `pyarabic.araby`. Here's an example:\n",
            "\n",
            "```python\n",
            "from pyarabic.araby import strip_tashkeel\n",
            "\n",
            "text = \"هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\"\n",
            "normalized_text = strip_tashkeel(text)\n",
            "print(normalized_text)\n",
            "```\n",
            "\n",
            "This will output:\n",
            "\n",
            "```\n",
            "hL zhBt 'l mkTbḥ?\n",
            "```\n",
            "\n",
            "Alternatively, you can also use the `dediac_ar()` function from the CAMeL Tools library, which performs both dediacritization and orthographic normalization:\n",
            "\n",
            "```python\n",
            "from camel_tools.utils.dediac import dediac_ar\n",
            "\n",
            "sentence = \"هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\"\n",
            "sentence_dediac = dediac_ar(sentence)\n",
            "print(sentence_dediac)\n",
            "```\n",
            "\n",
            "This will output:\n",
            "\n",
            "```\n",
            "hL zhBt il mkTbḤ?\n",
            "```\n",
            "\n",
            "In general, it's recommended to perform dediacritization before any other preprocessing steps, as mentioned in Document 3.\n",
            "==================================Source docs==================================\n",
            "Document 0------------------------------------------------------------\n",
            "'code' cell: '['import pyarabic.araby as araby', '', 'normalized_text=araby.strip_tashkeel(text)', 'print(normalized_text)']'\n",
            "Document 1------------------------------------------------------------\n",
            "'code' cell: '['import pyarabic.araby as araby', '', 'normalized_text = araby.strip_tashkeel(text)', 'print(normalized_text)']'\n",
            "Document 2------------------------------------------------------------\n",
            "'markdown' cell: '['## Dediacritization', '', 'Dediacritization is the process of removing Arabic diacritical marks. Diacritics increase data sparsity and so most Arabic NLP techniques ignore them. The example below shows how diacritics can be removed from Arabic text using the [`dediac_ar`](https://camel-tools.readthedocs.io/en/latest/api/utils/dediac.html#camel_tools.utils.dediac.dediac_ar) function:']'\n",
            "\n",
            "'code' cell: '['from camel_tools.utils.dediac import dediac_ar', '', 'sentence = \"هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\"', 'print(sentence)', '', 'sent_dediac = dediac_ar(sentence)', 'print(sent_dediac)']'\n",
            " with output: '['هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟', 'هل ذهبت إلى المكتبة؟']'\n",
            "\n",
            "'markdown' cell: '['# Disambiguation']'\n",
            "Document 3------------------------------------------------------------\n",
            "'markdown' cell: '['**NOTE:** It is advised to run this function on all text prior to any further preprocessing or use.']'\n",
            "\n",
            "'markdown' cell: '['### Orthographic Normalization', '', 'It is common for Arabic speakers to use shortcuts when typing Arabic text.', \"For example, the different variants of the letter alef ('ا', 'آ', 'أ', 'إ') may be typed as just 'ا'. Some of these substitutions can just be the result of typos. The presence of these variations can cause data sparsity and are usually normalized to a single form. Orthographic normalization is the process of converting letter variants or visually similar letters into a single form.\", '', 'We provide a collection of orthographic normalization functions in CAMeL Tools. The example below demonstrates a few of them.']'\n",
            "Document 4------------------------------------------------------------\n",
            "'المشروعيمكن', 'تقدير', 'ايرادات', 'المشروع', 'طريق', 'تقدير', 'كميه', 'المبيعات', 'خلال', 'فتره', 'تتراوح', 'ثلاث', 'وخمس', 'سنوات', 'او', 'اكثر', 'ويقوم', 'الخبير', 'بدراسه', 'الجدوي', 'بضرب', 'سعر', 'بيع', 'الوحده', 'الواحده', 'المنتج', 'عدد', 'الوحدات', 'المتوقع', 'بيعها', 'خلال', 'الفتره', 'وهنا', 'بد', 'التاكيد', 'علي', 'اهميه', 'تحديد', 'السعر', 'بالاعتماد', 'علي', 'اسعار',\n"
          ]
        }
      ],
      "source": [
        "print(\"==================================Answer==================================\")\n",
        "print(f\"{answer}\")\n",
        "print(\"==================================Source docs==================================\")\n",
        "for i, doc in enumerate(relevant_docs):\n",
        "    print(f\"Document {i}------------------------------------------------------------\")\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-telegram-bot==13.7"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UKUkTo83oEmJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "outputId": "0a8d239c-279e-44e4-f977-41d2d8cf38a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-telegram-bot==13.7\n",
            "  Downloading python_telegram_bot-13.7-py3-none-any.whl (490 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/490.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m481.3/490.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.1/490.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.7) (2024.2.2)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.7) (6.3.3)\n",
            "Collecting APScheduler==3.6.3 (from python-telegram-bot==13.7)\n",
            "  Downloading APScheduler-3.6.3-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.7) (2023.4)\n",
            "Collecting cachetools==4.2.2 (from python-telegram-bot==13.7)\n",
            "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.7) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.7) (1.16.0)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.7) (5.2)\n",
            "Installing collected packages: cachetools, APScheduler, python-telegram-bot\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.3\n",
            "    Uninstalling cachetools-5.3.3:\n",
            "      Successfully uninstalled cachetools-5.3.3\n",
            "Successfully installed APScheduler-3.6.3 cachetools-4.2.2 python-telegram-bot-13.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cachetools"
                ]
              },
              "id": "f99359a13fd24c19a321633f19a62e5c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from telegram.ext import Updater, MessageHandler, Filters,CommandHandler\n",
        "from transformers import Pipeline\n",
        "\n",
        "# Initialize list to store last four questions\n",
        "chat_history = []\n",
        "\n",
        "def handle_message(update, context):\n",
        "    global chat_history\n",
        "    question = update.message.text\n",
        "\n",
        "    # Add the current question to the chat history\n",
        "    chat_history.append(question)\n",
        "\n",
        "    # Keep only the last four questions\n",
        "    chat_history = chat_history[-4:]\n",
        "\n",
        "    # Build prompt using the last four questions\n",
        "    prompt = \"\\n\".join(chat_history)\n",
        "\n",
        "    # Get answer using chat history as context (replace with your implementation)\n",
        "    answer, relevant_docs = answer_with_rag(prompt, READER_LLM, VECTORDB, reranker=RERANKER)\n",
        "    update.message.reply_text(answer)\n",
        "    follow_up_question = \"Does your next question depend on my previous answer? (Click /YES or /NO).If you are finished, click /Exit.\"\n",
        "    update.message.reply_text(follow_up_question)\n",
        "\n",
        "def handle_exit(update, context):\n",
        "    \"\"\"\n",
        "    Handles the /exit command, sends a thank you message, and stops the bot.\n",
        "    \"\"\"\n",
        "    update.message.reply_text(\"Thank you for using the bot!.\")\n",
        "    context.bot.stop()  # Stops the bot from polling for new messages\n",
        "\n",
        "def start(update, context):\n",
        "  context.bot.send_message(chat_id=update.message.chat_id, text=\"Hi, I am your teach assistant I am here for any questions\")\n",
        "\n",
        "\n",
        "\n",
        "def yes(update, context):\n",
        "  context.bot.send_message(chat_id=update.message.chat_id, text=\"Enter your next question\")\n",
        "\n",
        "def no(update, context):\n",
        "  context.bot.send_message(chat_id=update.message.chat_id, text=\"Enter your next question\")\n",
        "  chat_history.clear()\n",
        "  print(chat_history)\n",
        "\n",
        "\n",
        "def main():\n",
        "    updater = Updater('YOUR_TOKEN', use_context=True)\n",
        "    dispatcher = updater.dispatcher\n",
        "\n",
        "    text_question_handler = MessageHandler(Filters.text & ~Filters.command, handle_message)\n",
        "    start_handler = CommandHandler('start', start)\n",
        "    yes_handler = CommandHandler('YES', yes)\n",
        "    no_handler = CommandHandler('NO', no)\n",
        "    exit_handler = CommandHandler('Exit', handle_exit)\n",
        "\n",
        "    dispatcher.add_handler(start_handler)\n",
        "    dispatcher.add_handler(text_question_handler)\n",
        "    dispatcher.add_handler(yes_handler)\n",
        "    dispatcher.add_handler(no_handler)\n",
        "    dispatcher.add_handler(exit_handler)\n",
        "\n",
        "    updater.start_polling()\n",
        "    updater.idle()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2539472d-6b76-4d32-803b-daa3d26dc0c8",
        "id": "3cPLAA3U1Y9G"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Retrieving documents...\n",
            "=> Reranking documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Generating answer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:telegram.ext.dispatcher:No error handlers are registered, logging exception.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telegram/ext/dispatcher.py\", line 555, in process_update\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telegram/ext/handler.py\", line 198, in handle_update\n",
            "  File \"<ipython-input-55-41f0accb0171>\", line 31, in handle_exit\n",
            "    context.bot.stop()  # Stops the bot from polling for new messages\n",
            "AttributeError: 'ExtBot' object has no attribute 'stop'\n"
          ]
        }
      ]
    }
  ]
}